{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 1: Exercises\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan HÃ¼bert**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Directory Management\n",
    "\n",
    "1. In the following code chunk, create an object called `edir` that has the fill path to the directory where you will store your work on these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the following code chunk, download the file `news_article.txt` from the data repo on the course GitHub, and save it into `edir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fun with Digital Text\n",
    "\n",
    "1. Read `news_article.txt` into Python as raw bytes, and then echo the first 10 bytes of the resulting `bytes` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to decode the `bytes` object using the UTF-8 and UTF-16 encoding standards and the `bytes.decode()` method. Is the file you read into Python UTF-8 or UTF-16 encoded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, use the `charset_normalizer` module to try to discover the encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now try to decode the `bytes` object using the best guess you get from the `charset_normalizer` module. Print the decoded text using the `print()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save a new version of the file called `news_article_utf8.txt`, where the text you've just decoded is re-encoded as UTF-8 when saving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Try to read the newly created file in to Python to verify that it is indeed encoded as UTF-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How many characters is in this file? How many bytes does it take to store it as a UTF-8 encoded file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Trump Tweet Data\n",
    "\n",
    "1. Load the sparse matrix, vocabular and the tweet corpus you made in a previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove any tweet posted before Trump became president. Hint: all US presidents are inaugurated at 12:00 Eastern US time on the 20th of January of their first year in office."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Trim the dataset features by dropping any token that does not occur in at least 5 documents, and at least 8 times total in the corpus. How many features did you trim out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Basic Text Manipulations\n",
    "\n",
    "1. Display Trump's first tweet as president. Do this in three chunks: one where you echo the tweet, one where you print the tweet, and one where you pretty print the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the tweet posted at 12:55 pm Eastern US time on the day that Trump was first inaugurated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenise the tweet using whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Replace any emojis with a descriptive placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pre-process the tweet by: standardising the cases as appropriate, cleaning out non-words, punctuation and numbers, and any other step you need to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load English stop words from the `nltk` module and remove any stop word from the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Stem words using the Snowball stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Using a `Counter` object, calculate the term frequencies of this tweet. Echo the `Counter` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Turn this `Counter` object into a row of a Pandas `DataFrame` object and into a row of a SciPy sparse array object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
